# Utility for computing a cache that maps the prefix of any word in a word list
# to the subword sequences that would need to be queried to compute the
# character distribution over the next character.

import argparse
import os.path
import sys
import time
from transformers import AutoTokenizer
from collections import defaultdict
from typing import List
import itertools

def sequence_to_text_len(sequence: List[int]) -> int:
    """
    Compute the text length of a subword sequence
    :param sequence: List of subword token IDs
    :return: Integer length of the text it represents
    """
    result = 0
    for token_id in sequence:
        result += len(index_to_word[token_id])
    return result

def sequence_to_text(sequence: List[int], surround: bool = False) -> str:
    """
    Converts a subword token sequence into a text string
    :param sequence: List of subword token IDs
    :param surround: Surround subword tokens with ()'s
    :return: string representation of the subword token IDs
    """
    result = ""
    for token_id in sequence:
        if surround:
            result += f"({index_to_word[token_id]})"
        else:
            result += index_to_word[token_id]
    return result

def sequences_to_text(sequences: List[List[int]]) -> str:
    """
    Converts a list of subword token sequences into a text string
    :param sequence: List of list of subword token IDs
    :return: string representation of the list of lists
    """
    result = ""
    for sequence in sequences:
        result += "[" + sequence_to_text(sequence, True) + "] "
    return result

# TODO: can delete eventually
def sequences_longer_than(text: str) -> List[List[int]]:
    """
    Search for all subword token sequences that result in text that
    matches the given prefix and are strictly longer.
    :param text: Prefix of a word in our vocabulary
    :return: List of lists of subword token IDs
    """

    # Where we put hypotheses that have become long enough
    finished = []

    # Starting point for the search is an empty list of lists
#    current_hypos = [[=] for seq in prefix_to_ids[text]]
    current_hypos = [[]]
    next_hypos = []
    print(f"START with text {text}")

    while len(current_hypos) > 0:
        #print(f"LOOP top: current_hypos {sequences_to_text(current_hypos)}")
        # Check whether hypotheses to see if they have finished
        for hypo in current_hypos:
#            print(f"hypo {hypo}")
            text_len = sequence_to_text_len(hypo)
            if text_len > len(text):
                # We don't need to extend this sequence any further since it now generates text that
                # is strictly longer that the prefix of the word in our vocabulary.
                finished += hypo,
               #print(f"LOOP, finished {hypo}")
            else:
                # This sequence needs to be longer.
                # Extend by all subword tokens that are compatible with the text
                # after the text generated by the current sequence.
                remaining_context = text[text_len:]
                vocab = []
                extra_vocab = []
                if len(remaining_context) == 0:
                    # There is no remaining context thus all subword tokens that are valid under our symbol set
                    # should be considered when computing the probability of the next character.
                    for token_id in valid_vocab:
                        next_hypos.append(hypo.copy() + [token_id])
                else:
                    if remaining_context in prefix_to_ids:
                        # We have a list of subword tokens that match the remaining text.
                        # They could be the same length as the remaining text or longer and have the remaining text as a prefix.
                        for token_id in prefix_to_ids[remaining_context]:
                            next_hypos.append(hypo.copy() + [token_id])

                    # We may need to use a subword token that doesn't completely consume the remaining text.
                    # Find these by tokenizing all possible lengths of text starting from the current position.
                    for i in range(1, len(remaining_context)):
                        tokenization = tokenizer.encode(text[text_len:text_len + i])
                        # Ignore tokenizations involving multiple tokens since they involve an ID we would have already added.
                        if len(tokenization) == 1:
                            next_hypos.append(hypo.copy() + [tokenization[0]])

        #print(f"DONE: next_hypos {sequences_to_text(next_hypos)}")
        current_hypos = next_hypos
        next_hypos = []

    return finished

def sequences_generating(text: str) -> List[List[int]]:
    """
    Search for all subword token sequences that given one more subword token
    can generate text that is longer that the given string.
    :param text: Prefix of a word in our vocabulary
    :return: List of tuples of subword token IDs
    """

    # List we put the finished sequences
    result = []

    # Starting point for the search is an empty list of lists
    current_hypos = [[]]
    next_hypos = []
    print(f"START with text {text}")

    while len(current_hypos) > 0:
        #print(f"LOOP top: current_hypos {sequences_to_text(current_hypos)}")
        # Check whether hypotheses to see if they have finished
        for hypo in current_hypos:
            text_len = sequence_to_text_len(hypo)
            if text_len > len(text):
                # We don't need to extend this sequence any further since it now generates text that
                # is strictly longer that the prefix of the word in our vocabulary.
                # Remove the last token since at inference time we will generate all following tokens.
                hypo.pop()

                # Only add if it isn't already in the result of generating sequences.
                if hypo not in result:
                    result.append(hypo)
            else:
                # This sequence needs to be longer.
                # Extend by all subword tokens that are compatible with the text
                # after the text generated by the current sequence.
                remaining_context = text[text_len:]
                if len(remaining_context) == 0:
                    # There is no remaining context thus all subword tokens that are valid under our symbol set
                    # should be considered when computing the probability of the next character.
                    for token_id in valid_vocab:
                        next_hypos.append(hypo.copy() + [token_id])
                else:
                    if remaining_context in prefix_to_ids:
                        # We have a list of subword tokens that match the remaining text.
                        # They could be the same length as the remaining text or longer and have the remaining text as a prefix.
                        for token_id in prefix_to_ids[remaining_context]:
                            next_hypos.append(hypo.copy() + [token_id])

                    # We may need to use a subword token that doesn't completely consume the remaining text.
                    # Find these by tokenizing all possible lengths of text starting from the current position.
                    for num_chars in range(1, len(remaining_context)):
                        tokenization = tokenizer.encode(text[text_len:text_len + num_chars])
                        # Ignore tokenizations involving multiple tokens since they involve an ID we would have already added.
                        if len(tokenization) == 1:
                            next_hypos.append(hypo.copy() + [tokenization[0]])

        current_hypos = next_hypos
        next_hypos = []

    return result

if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument("--words",
                        type=str,
                        required=True,
                        help="Word list filename")
    parser.add_argument("--model-name",
                        required=True,
                        help="Language model name")
    parser.add_argument("--lower",
                        action="store_true",
                        help="Lowercase words and subword tokens")
    parser.add_argument("--symbols",
                        default="abcdefghijklmnopqrstuvwxyz' ",
                        help="valid symbols")
    parser.add_argument("--drop-first-token",
                        action="store_true",
                        help="Drop first subword token from tokenizer")

    args = parser.parse_args()
    start_time_ns = time.time_ns()

    # First get the words we are going to compute over
    if not os.path.isfile(args.words):
        print(f"ERROR: can't find filename: '{args.words}'")
        sys.exit(1)

    if (args.model_name.startswith("facebook/opt") or "Llama-3.1" in args.model_name) and \
        not args.drop_first_token:
        print(f"WARNING: detected OPT or Llama model, recommend settings --drop-first-token")

    # Create a set of the valid characters we plan to use during predictions
    # that make use of the cache we are generating.
    valid_symbols = set(args.symbols)
    print(f"{len(valid_symbols)} valid symbols: {args.symbols}")

    seen_words = set()
    words = []
    dropped_words = 0

    with open(args.words) as file:
        for word in file:
            word = word.strip()
            if args.lower:
                word = word.lower()
            # Don't include words more than once
            if word not in seen_words:
                # Only keep words where all characters are in our symbol set
                if 0 not in [ch in valid_symbols for ch in word]:
                    words.append(word)
                else:
                    dropped_words += 1
    print(f"Dropped {dropped_words} words")

    if len(words) == 0:
        print(f"ERROR: no words found in filename: '{args.words}'")
        sys.exit(1)

    print(f"Computing cache for {len(words)} words...")

    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=False)
    vocab_size = tokenizer.vocab_size
    print(f"Language model vocab size: {vocab_size}")

    # Create list from subword integer index to the actual token's text.
    # Track a mixed case and a lowercase version.
    index_to_word = []
    index_to_word_lower = []
    for i in range(vocab_size):
        word = tokenizer.decode([i])
        index_to_word += word,
        index_to_word_lower += word.lower(),

    # This dictionary will map prefixes of words to valid subword token IDs
    # that match the prefix text. For example under facebook/opt-125m:
    # prefix_to_ids["cyclo"] = [47495, 49484]
    # index_to_word[prefix_to_ids["cyclo"][0]] = cyclop
    # index_to_word[prefix_to_ids["cyclo"][1]] = cyclopedia
    prefix_to_ids = defaultdict(list)

    # Also track a set with the token IDs valid under our symbol set and lowercase setting
    valid_vocab = set()
    dropped_subwords = 0
    for i in range(vocab_size):
        token_text = tokenizer.decode([i])
        # Note: if --lower is set then we lowercase tokens here so that any token matching
        # the character ignoring case count during the search.
        if args.lower:
            token_text = token_text.lower()
        if 0 not in [ch in valid_symbols for ch in token_text]:
            for j in range(len(token_text)):
                prefix_to_ids[token_text[0:j + 1]] += i,
            valid_vocab.add(i)
        else:
            dropped_subwords += 1
            #print(f"DROPPED: {token_text}")
    print(f"Valid subwords {len(prefix_to_ids)}, dropped {dropped_subwords}")

    # Create a dictionary where the keys are a prefix of a word in the vocabulary
    # and the value is a list of all subword token sequences that are one subword
    # token short of generating the prefix. This includes the entire word itself
    # since we need to predict the following space.
    # For example:
    # WORD: catching
    # PREFIX: catchi

    prefix_to_sequences = {}
    prefix_count = 0
    for word in words:
        for i in range(1, len(word) + 1):
            prefix = word[:i]
            prefix_count += 1
            print(f"prefix {prefix}")
            generating = sequences_generating(text = prefix)
            print(f"generating {sequences_to_text(generating)}, size = {len(generating)}")

    print(f"Prefix count: {prefix_count}")

    end_time_ns = time.time_ns()
    print(f"Total time: {(end_time_ns - start_time_ns) / 1e9 :.3f}")