# Utility for computing a cache that maps the prefix of any word in a word list
# to the subword sequences that would need to be queried to compute the
# character distribution over the next character.

import argparse
import os.path
import sys
import time
from transformers import AutoTokenizer
from collections import defaultdict
from typing import List

def sequence_to_text_len(sequence: List[int]) -> int:
    """
    Compute the text length of a subword sequence
    :param sequence: List of subword token IDs
    :return: Integer length of the text it represents
    """
    result = 0
    for token_id in sequence:
        result += len(index_to_word[token_id])
    return result

def sequence_to_text(sequence: List[int], surround: bool = False) -> str:
    """
    Converts a subwork token sequence into a text string
    :param sequence: List of subword token IDs
    :param surround: Surround subword tokens with ()'s
    :return: string representation of the subword token IDs
    """
    result = ""
    for token_id in sequence:
        if surround:
            result += f"({index_to_word[token_id]})"
        else:
            result += index_to_word[token_id]
    return result

def sequences_longer_than(text: str) -> List[List[int]]:
    """
    Search for all subword token sequences that result in text that
    matches the given prefix and are strictly longer.
    :param text: Prefix of a word in our vocabulary
    :return: List of lists of subword token IDs
    """

    # Where we put hypotheses that have become long enough
    finished = []

    # Starting point for the search is an empty list of lists
#    current_hypos = [[=] for seq in prefix_to_ids[text]]
    current_hypos = [[]]
    next_hypos = []
    print(f"START with {len(current_hypos)}")

    while len(current_hypos) > 0:
        print(f"LOOP top: current {len(current_hypos)}")
        # Check whether hypotheses to see if they have finished
        for hypo in current_hypos:
            print(f"hypo {hypo}")
            text_len = sequence_to_text_len(hypo)
            if text_len > len(text):
                # We don't need to extend this sequence any further
                finished += hypo,
            else:
                # This sequence needs to be longer.
                # Extend by all subword tokens that are compatible with the text
                # after the text generated by the current sequence.
                remaining_text = text[text_len:]
                if len(remaining_text) > 0:
                    remaining_tokens = tokenizer.encode(remaining_text)
                    # Some models automatically insert a start symbol at the start which we need to drop
                    if args.drop_first_token:
                        remaining_tokens = remaining_tokens[1:]
                    print(f"text '{text}', remaining text '{remaining_text}', remaining tokens '{remaining_tokens}'")
                    if len(remaining_tokens) == 1:
                        next_hypos.append(hypo.copy() + remaining_tokens)
                else:
                    # Sequence thus far has exactly consumed the characters in the text.
                    # In this case, all possible valid token IDs under the symbol set
                    # could come after.
                    for valid_id in valid_ids:
                        next_hypos.append(hypo.copy() + [valid_id])
        print(f"LOOP done, {len(next_hypos)}")

        current_hypos = next_hypos
        next_hypos = []

    return finished

if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument("--words",
                        type=str,
                        required=True,
                        help="Word list filename")
    parser.add_argument("--model-name",
                        required=True,
                        help="Language model name")
    parser.add_argument("--lower",
                        action="store_true",
                        help="Lowercase words and subword tokens")
    parser.add_argument("--symbols",
                        default="abcdefghijklmnopqrstuvwxyz' ",
                        help="valid symbols")
    parser.add_argument("--drop-first-token",
                        action="store_true",
                        help="Drop first subword token from tokenizer")

    args = parser.parse_args()
    start_time_ns = time.time_ns()

    # First get the words we are going to compute over
    if not os.path.isfile(args.words):
        print(f"ERROR: can't find filename: '{args.words}'")
        sys.exit(1)

    if (args.model_name.startswith("facebook/opt") or "Llama-3.1" in args.model_name) and \
        not args.drop_first_token:
        print(f"WARNING: detected OPT or Llama model, recommend settings --drop-first-token")

    # Create a set of the valid characters we plan to use during predictions
    # that make use of the cache we are generating.
    valid_symbols = set(args.symbols)
    print(f"{len(valid_symbols)} valid symbols: {args.symbols}")

    seen_words = set()
    words = []
    dropped_words = 0

    with open(args.words) as file:
        for word in file:
            word = word.strip()
            if args.lower:
                word = word.lower()
            # Don't include words more than once
            if word not in seen_words:
                # Only keep words where all characters are in our symbol set
                if 0 not in [ch in valid_symbols for ch in word]:
                    words.append(word)
                else:
                    dropped_words += 1
    print(f"Dropped {dropped_words} words")

    if len(words) == 0:
        print(f"ERROR: no words found in filename: '{args.words}'")
        sys.exit(1)

    print(f"Computing cache for {len(words)} words...")

    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=False)
    vocab_size = tokenizer.vocab_size
    print(f"Language model vocab size: {vocab_size}")

    # Create list from subword integer index to the actual token's text.
    index_to_word = []
    for i in range(vocab_size):
        word = tokenizer.decode([i])
        if args.lower:
            word = word.lower()
        index_to_word += word,

    # This dictionary will map prefixes of words to valid subword token IDs
    # that match the prefix text. For example under facebook/opt-125m:
    # prefix_to_ids["cyclo"] = [47495, 49484]
    # index_to_word[prefix_to_ids["cyclo"][0]] = cyclop
    # index_to_word[prefix_to_ids["cyclo"][1]] = cyclopedia
    prefix_to_ids = defaultdict(list)

    # Also track a set with the token IDs valid under our symbol set and lowercase setting
    valid_ids = set()
    dropped_subwords = 0
    for i in range(vocab_size):
        token_text = tokenizer.decode([i])
        if args.lower:
            token_text = token_text.lower()
        if 0 not in [ch in valid_symbols for ch in token_text]:
            for j in range(len(token_text)):
                prefix_to_ids[token_text[0:j + 1]] += i,
            valid_ids.add(i)
        else:
            dropped_subwords += 1
            #print(f"DROPPED: {token_text}")
    print(f"Valid subwords {len(prefix_to_ids)}, dropped {dropped_subwords}")

    # Create a dictionary where the keys are a prefix of a word in the vocabulary
    # and the value is a list of all subword token sequences that are one subword
    # token short of generating the prefix. This includes the entire word itself
    # since we need to predict the following space.
    # For example:
    # WORD: catching
    # PREFIX: catchi

    prefix_to_sequences = {}
    prefix_count = 0
    for word in words:
        for i in range(1, len(word) + 1):
            prefix = word[:i]
            prefix_count += 1
            print(f"prefix {prefix}")
            longer = sequences_longer_than(text = prefix)
            print(f"longer {len(longer)}")
            print(f"longer first {sequence_to_text(sequence=longer[0], surround=True)}")
            print(f"longer middle {sequence_to_text(sequence=longer[len(longer) // 2], surround=True)}")
            print(f"longer last {sequence_to_text(sequence=longer[-1], surround=True)}")

    print(f"Prefix count: {prefix_count}")

    end_time_ns = time.time_ns()
    print(f"Total time: {(end_time_ns - start_time_ns) / 1e9 :.3f}")