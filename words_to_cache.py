# Utility for computing a cache that maps the prefix of any word in a word list
# to the subword sequences that would need to be queried to compute the
# character distribution over the next character.

import argparse
import os.path
import sys
import time
from transformers import AutoTokenizer
from collections import defaultdict
from typing import List
from typing import Final

def sequence_to_text_len(sequence: List[int]) -> int:
    """
    Compute the text length of a subword sequence
    :param sequence: List of subword token IDs
    :return: Integer length of the text it represents
    """
    result = 0
    for token_id in sequence:
        result += len(index_to_word[token_id])
    return result

def sequence_to_text(sequence: List[int], surround: bool = False) -> str:
    """
    Converts a subword token sequence into a text string
    :param sequence: List of subword token IDs
    :param surround: Surround subword tokens with ()'s
    :return: string representation of the subword token IDs
    """
    result = ""
    for token_id in sequence:
        if surround:
            result += f"({index_to_word[token_id]})"
        else:
            result += index_to_word[token_id]
    return result

def sequences_to_text(sequences: List[List[int]]) -> str:
    """
    Converts a list of subword token sequences into a text string
    :param sequences: List of list of subword token IDs
    :return: string representation of the list of lists
    """
    result = ""
    for sequence in sequences:
        result += "[" + sequence_to_text(sequence, True) + "] "
    return result

def sequences_generating(text: str) -> list[tuple]:
    """
    Search for all subword token sequences that given one more subword token
    can generate text that is longer that the given string.
    :param text: Prefix of a word in our vocabulary
    :return: List of tuples of subword token IDs
    """

    # List we put the finished sequences
    # Store as a set until the very end for faster checking if already in results
    results = set()

    # Starting point for the search is an empty list of lists.
    # Hypothesis format is ([token sequence list], text len of sequence)
    current_hypos = [([], 0)]
    next_hypos = []

    SEQ: Final[int] = 0
    LEN: Final[int] = 1

    while len(current_hypos) > 0:
        #print(f"LOOP top: current_hypos {sequences_to_text(current_hypos)}")
        # Check whether hypotheses to see if they have finished
        for hypo in current_hypos:
            # Extend by all subword tokens that are compatible with the text
            # after the text generated by the current sequence.
            remaining_context = text[hypo[LEN]:]
            if len(remaining_context) == 0:
                # There is no remaining context thus all subword tokens that are valid under our symbol set
                # should be considered when computing the probability of the next character.
                for token_id in valid_vocab:
                    # New hypothesis has the extra token and increases by its text length.
                    # See if this makes it longer in which case we just save it off in the results.
                    # Note: this code is repeated twice more in the else, cleanup somehow?
                    new_len = hypo[LEN] + len(index_to_word[token_id])
                    if new_len > len(text):
                        t = tuple(hypo[SEQ])
                        if t not in results:
                            results.add(t)
                    else:
                        next_hypos.append((hypo[SEQ] + [token_id], new_len))
            else:
                if remaining_context in prefix_to_ids:
                    # We have a list of subword tokens that match the remaining text.
                    # They could be the same length as the remaining text or longer and have the remaining text as a prefix.
                    for token_id in prefix_to_ids[remaining_context]:
                        new_len = hypo[LEN] + len(index_to_word[token_id])
                        if new_len > len(text):
                            t = tuple(hypo[SEQ])
                            if t not in results:
                                results.add(t)
                        else:
                            next_hypos.append((hypo[SEQ] + [token_id], new_len))

                # We may need to use a subword token that doesn't completely consume the remaining text.
                # Find these by tokenizing all possible lengths of text starting from the current position.
                for num_chars in range(1, len(remaining_context)):
                    tokenization = tokenizer.encode(text[hypo[LEN]:hypo[LEN] + num_chars])
                    # Ignore tokenizations involving multiple tokens since they involve an ID we would have already added.
                    if len(tokenization) == 1:
                        token_id = tokenization[0]
                        new_len = hypo[LEN] + len(index_to_word[token_id])
                        if new_len > len(text):
                            t = tuple(hypo[SEQ])
                            if t not in results:
                                results.add(t)
                        else:
                            next_hypos.append((hypo[SEQ] + [token_id], new_len))

        current_hypos = next_hypos
        next_hypos = []

    return list(results)

if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument("--words",
                        type=str,
                        required=True,
                        help="Word list filename")
    parser.add_argument("--model-name",
                        required=True,
                        help="Language model name")
    parser.add_argument("--lower",
                        action="store_true",
                        help="Lowercase words and subword tokens")
    parser.add_argument("--symbols",
                        default="abcdefghijklmnopqrstuvwxyz' ",
                        help="valid symbols")
    parser.add_argument("--drop-first-token",
                        action="store_true",
                        help="Drop first subword token from tokenizer")

    args = parser.parse_args()
    start_time_ns = time.time_ns()

    # First get the words we are going to compute over
    if not os.path.isfile(args.words):
        print(f"ERROR: can't find filename: '{args.words}'")
        sys.exit(1)

    if (args.model_name.startswith("facebook/opt") or "Llama-3.1" in args.model_name) and \
        not args.drop_first_token:
        print(f"WARNING: detected OPT or Llama model, recommend settings --drop-first-token")

    # Create a set of the valid characters we plan to use during predictions
    # that make use of the cache we are generating.
    valid_symbols = set(args.symbols)
    print(f"{len(valid_symbols)} valid symbols: {args.symbols}")

    seen_words = set()
    words = []
    dropped_words = 0

    longest_word = ""
    with open(args.words) as file:
        for word in file:
            word = word.strip()
            if args.lower:
                word = word.lower()
            # Don't include words more than once
            if word not in seen_words:
                # Only keep words where all characters are in our symbol set
                if 0 not in [ch in valid_symbols for ch in word]:
                    words.append(word)
                    if len(word) > len(longest_word):
                        longest_word = word
                else:
                    dropped_words += 1
    print(f"Dropped {dropped_words} words")
    print(f"Longest word: {longest_word}")

    if len(words) == 0:
        print(f"ERROR: no words found in filename: '{args.words}'")
        sys.exit(1)

    print(f"Computing cache for {len(words)} words...")

    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=False)
    vocab_size = tokenizer.vocab_size
    print(f"Language model vocab size: {vocab_size}")

    # Create list from subword integer index to the actual token's text.
    # Track a mixed case and a lowercase version.
    index_to_word = []
    index_to_word_lower = []
    for i in range(vocab_size):
        word = tokenizer.decode([i])
        index_to_word += word,
        index_to_word_lower += word.lower(),

    # This dictionary will map prefixes of words to valid subword token IDs
    # that match the prefix text. For example under facebook/opt-125m:
    # prefix_to_ids["cyclo"] = [47495, 49484]
    # index_to_word[prefix_to_ids["cyclo"][0]] = cyclop
    # index_to_word[prefix_to_ids["cyclo"][1]] = cyclopedia
    prefix_to_ids = defaultdict(list)

    # Also track a set with the token IDs valid under our symbol set and lowercase setting
    valid_vocab = set()
    dropped_subwords = 0
    for i in range(vocab_size):
        token_text = tokenizer.decode([i])
        # Note: if --lower is set then we lowercase tokens here so that any token matching
        # the character ignoring case count during the search.
        if args.lower:
            token_text = token_text.lower()
        if 0 not in [ch in valid_symbols for ch in token_text]:
            for j in range(len(token_text)):
                prefix_to_ids[token_text[0:j + 1]] += i,
            valid_vocab.add(i)
        else:
            dropped_subwords += 1
    print(f"Valid subwords {len(prefix_to_ids)}, dropped {dropped_subwords}")

    # Create a dictionary where the keys are a prefix of a word in the vocabulary
    # and the value is a list of all subword token sequences that are one subword
    # token short of generating the prefix. This includes the entire word itself
    # since we need to predict the following space. We also include a leading space
    # since at inference time we backoff to this space and generate forward.
    # For example:
    # WORD: catching
    # PREFIX: catchi

    prefix_to_sequences = {}
    prefix_count = 0
    for word in words:
        for i in range(1, len(word) + 1):
            prefix = word[:i]
            prefix_count += 1
            print(f"prefix {prefix}")
            generating = list(sequences_generating(text = prefix))
            print(f"generating {sequences_to_text(generating)}, size = {len(generating)}")
            print(f"generating, size = {len(generating)}")

    print(f"Prefix count: {prefix_count}")

    end_time_ns = time.time_ns()
    print(f"Total time: {(end_time_ns - start_time_ns) / 1e9 :.3f}")